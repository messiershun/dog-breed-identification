{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6acd2a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as trans\n",
    "from PIL import Image\n",
    "import torchvision.datasets as dsets\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ef23b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label=pd.read_csv('dog-breed-identification/labelslist.csv',header=None)#加载标签类别号序列对\n",
    "labelslist=label.values.tolist()[0]\n",
    "label_dictionary=dict(zip(labelslist,range(120)))\n",
    "\n",
    "trpiclabels=pd.read_csv('dog-breed-identification/labels.csv')#加载图片标签序列对\n",
    "trpiclabels=dict(trpiclabels.values.tolist())\n",
    "\n",
    "tenames=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5938fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_data():\n",
    "    train_image_label=[]\n",
    "    verify_image_label=[]\n",
    "    train_transform = trans.Compose([\n",
    "        trans.ToPILImage(),\n",
    "        trans.Resize((256, 256)),\n",
    "        trans.RandomCrop((224, 224)), #训练集中的数据增强操作\n",
    "        trans.ToTensor()\n",
    "    ])\n",
    "    path='dog-breed-identification/train'\n",
    "    pic=os.listdir(path)#训练图片集\n",
    "    \n",
    "    for i in pic:\n",
    "        name=i.split('.')\n",
    "        pathpic=path+'/'+i\n",
    "        img = Image.open(pathpic).convert('RGB')\n",
    "        img_np = np.array(img)\n",
    "        img_tensor=train_transform(img_np)\n",
    "        l=label_dictionary[trpiclabels[name[0]]]\n",
    "        if random.random()<0.9:\n",
    "            train_image_label.append((img_tensor,l))\n",
    "        else:\n",
    "            verify_image_label.append((img_tensor,l))\n",
    "    return train_image_label,verify_image_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d950d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_data():\n",
    "    test_image_label=[]\n",
    "    train_transform = trans.Compose([\n",
    "        trans.ToPILImage(),\n",
    "        trans.Resize((224, 224)),\n",
    "        trans.ToTensor()\n",
    "    ])\n",
    "    path='dog-breed-identification/test'\n",
    "    pic=os.listdir(path)#测试图片集\n",
    "    for i in pic:\n",
    "        name=i.split('.')\n",
    "        tenames.append(name[0])\n",
    "        pathpic=path+'/'+i\n",
    "        img = Image.open(pathpic).convert('RGB')\n",
    "        img_np = np.array(img)\n",
    "        img_tensor=train_transform(img_np)\n",
    "        test_image_label.append(img_tensor)\n",
    "    return test_image_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c591871f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set,verify_set=read_train_data()\n",
    "test_set=read_test_data()\n",
    "\n",
    "train_dl = DataLoader(train_set, batch_size = 50, shuffle = True, drop_last = False)\n",
    "verify_dl = DataLoader(verify_set, batch_size = 50, shuffle = True, drop_last = False)\n",
    "test_dl = DataLoader(test_set, batch_size = 50, shuffle = False, drop_last = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa2127ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9217 1005 10357\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([50, 3, 224, 224])\n",
      "torch.Size([7, 3, 224, 224])\n",
      "208\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set),len(verify_set),len(test_set))\n",
    "i=0\n",
    "for a in test_dl:\n",
    "    print(a.shape)\n",
    "    i=i+1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7be09b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,input_size,output_size,useds):\n",
    "        super(BasicBlock,self).__init__()\n",
    "        if useds:\n",
    "            self.conv1=nn.Conv2d(input_size, output_size, 3, 2, 1, bias=False)\n",
    "        else:\n",
    "            self.conv1=nn.Conv2d(input_size, output_size, 3, 1, 1, bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(output_size)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        self.conv2=nn.Conv2d(output_size, output_size, 3, 1, 1, bias=False)\n",
    "        self.bn2=nn.BatchNorm2d(output_size)\n",
    "        self.useds=useds\n",
    "        if useds:\n",
    "            self.downsample=nn.Sequential(nn.Conv2d(input_size, output_size, 1, 2, bias=False),\n",
    "                                          nn.BatchNorm2d(output_size)\n",
    "                                         )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        o=self.relu(self.bn1(self.conv1(x)))\n",
    "        o=self.bn2(self.conv2(o))\n",
    "        if self.useds:\n",
    "            o=o+self.downsample(x)\n",
    "        o=self.relu(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a421bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myresnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(myresnet,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3, 64, 7, 2, 3, bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "        self.maxpool=nn.MaxPool2d(3, 2, 1, 1, ceil_mode=False)\n",
    "        self.layer1=nn.Sequential(BasicBlock(64,64,False),\n",
    "                                  BasicBlock(64,64,False)\n",
    "                                 )\n",
    "        self.layer2=nn.Sequential(BasicBlock(64,128,True),\n",
    "                                  BasicBlock(128,128,False)\n",
    "                                 )\n",
    "        self.layer3=nn.Sequential(BasicBlock(128,256,True),\n",
    "                                  BasicBlock(256,256,False)\n",
    "                                 )\n",
    "        self.layer4=nn.Sequential(BasicBlock(256,512,True),\n",
    "                                  BasicBlock(512,512,False)\n",
    "                                 )\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.fc=nn.Linear(512, 120)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        o=self.conv1(x)\n",
    "        o=self.bn1(o)\n",
    "        o=self.relu(o)\n",
    "        o=self.maxpool(o)\n",
    "        \n",
    "        o=self.layer1(o)\n",
    "        o=self.layer2(o)\n",
    "        o=self.layer3(o)\n",
    "        o=self.layer4(o)\n",
    "        \n",
    "        o=self.avgpool(o)\n",
    "        o=o.view(-1,512)\n",
    "        o=self.fc(o)\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f43922cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myresnet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=myresnet().to(device)\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr = 0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c90650",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model,criterion,dataloader):\n",
    "    net.eval()#因为使用了dropout，所以需要在评估时用这个函数让网络不再dropout\n",
    "    loss = 0#预测偏差值\n",
    "    accuracy = 0#预测准确率\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        logits = model(batch_x)\n",
    "        error = criterion(logits,batch_y)\n",
    "        loss += error.item()\n",
    "        probs,pred_y = logits.data.max(dim = 1)\n",
    "        accuracy += (pred_y == batch_y.data).sum().double()/batch_y.size(0)\n",
    "    loss /= len(dataloader)\n",
    "    accuracy = accuracy*100.0/len(dataloader)\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "417abd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO.0 train error: 4.4e+00, train acc: 4.62\t verify error: 4.4e+00, verify acc: 3.90\n",
      "NO.1 train error: 4.7e+00, train acc: 1.50\t verify error: 4.7e+00, verify acc: 1.33\n",
      "NO.2 train error: 4.7e+00, train acc: 1.50\t verify error: 4.8e+00, verify acc: 0.95\n",
      "NO.3 train error: 4.5e+00, train acc: 2.70\t verify error: 4.6e+00, verify acc: 2.10\n",
      "NO.4 train error: 4.4e+00, train acc: 4.06\t verify error: 4.5e+00, verify acc: 3.52\n",
      "NO.5 train error: 4.3e+00, train acc: 5.21\t verify error: 4.3e+00, verify acc: 3.62\n",
      "NO.6 train error: 4.2e+00, train acc: 6.15\t verify error: 4.3e+00, verify acc: 4.48\n",
      "NO.7 train error: 4.1e+00, train acc: 7.00\t verify error: 4.3e+00, verify acc: 6.00\n",
      "NO.8 train error: 4.0e+00, train acc: 8.53\t verify error: 4.2e+00, verify acc: 6.38\n",
      "NO.9 train error: 3.9e+00, train acc: 10.59\t verify error: 4.1e+00, verify acc: 6.76\n",
      "NO.10 train error: 3.6e+00, train acc: 14.67\t verify error: 4.0e+00, verify acc: 7.71\n",
      "NO.11 train error: 3.4e+00, train acc: 17.49\t verify error: 4.0e+00, verify acc: 9.24\n",
      "NO.12 train error: 3.4e+00, train acc: 18.94\t verify error: 4.0e+00, verify acc: 9.52\n",
      "NO.13 train error: 3.1e+00, train acc: 22.72\t verify error: 4.1e+00, verify acc: 9.14\n",
      "NO.14 train error: 2.8e+00, train acc: 29.67\t verify error: 4.1e+00, verify acc: 12.10\n",
      "NO.15 train error: 2.3e+00, train acc: 41.63\t verify error: 4.2e+00, verify acc: 11.43\n",
      "NO.16 train error: 1.7e+00, train acc: 54.31\t verify error: 4.5e+00, verify acc: 11.52\n",
      "NO.17 train error: 1.1e+00, train acc: 72.28\t verify error: 5.2e+00, verify acc: 10.76\n",
      "NO.18 train error: 6.1e-01, train acc: 83.60\t verify error: 6.3e+00, verify acc: 11.71\n",
      "NO.19 train error: 3.7e-01, train acc: 90.42\t verify error: 7.0e+00, verify acc: 9.52\n",
      "NO.20 train error: 2.3e-01, train acc: 94.03\t verify error: 9.0e+00, verify acc: 9.14\n",
      "NO.21 train error: 1.5e-01, train acc: 95.89\t verify error: 9.1e+00, verify acc: 9.81\n",
      "NO.22 train error: 1.1e-01, train acc: 96.73\t verify error: 9.8e+00, verify acc: 8.95\n",
      "NO.23 train error: 1.4e-01, train acc: 95.97\t verify error: 1.1e+01, verify acc: 8.76\n",
      "NO.24 train error: 1.5e-01, train acc: 95.82\t verify error: 1.0e+01, verify acc: 9.81\n",
      "NO.25 train error: 4.5e-02, train acc: 98.75\t verify error: 1.1e+01, verify acc: 10.19\n",
      "NO.26 train error: 8.8e-02, train acc: 97.57\t verify error: 9.9e+00, verify acc: 8.95\n",
      "NO.27 train error: 9.6e-02, train acc: 97.21\t verify error: 1.0e+01, verify acc: 8.76\n",
      "NO.28 train error: 8.0e-02, train acc: 97.59\t verify error: 1.1e+01, verify acc: 9.43\n",
      "NO.29 train error: 7.1e-02, train acc: 98.23\t verify error: 9.9e+00, verify acc: 8.67\n"
     ]
    }
   ],
   "source": [
    "nepochs = 30\n",
    "for epoch in range(nepochs):\n",
    "    #net.train()#使dropout发挥作用\n",
    "    for batch_x,batch_y in train_dl:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = net(batch_x)\n",
    "        error = criterion(logits,batch_y)#有平均\n",
    "        error.backward()\n",
    "        optimizer.step()\n",
    "    tr_loss, tr_acc = eval(net,criterion,train_dl)\n",
    "    ve_loss, ve_acc = eval(net,criterion,verify_dl)\n",
    "    print('NO.%d train error: %.1e, train acc: %.2f\\t verify error: %.1e, verify acc: %.2f'%(epoch,tr_loss,tr_acc,ve_loss,ve_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "046750db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ma-user/anaconda3/envs/PyTorch-1.8/lib/python3.7/site-packages/ipykernel/__main__.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "for i in test_dl:\n",
    "    batch_x=i.to(device)\n",
    "    #print(batch_x.shape)\n",
    "    logits = net(batch_x)\n",
    "    result1=nn.functional.softmax(logits)\n",
    "    if a!=0:\n",
    "        result=torch.cat([result,result1.detach()],dim=0)\n",
    "    else:\n",
    "        result=result1.detach()\n",
    "        a+=1\n",
    "result=result.to('cpu').numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2fc5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result=result[:-43,:]\n",
    "#result1.shape\n",
    "pd.DataFrame(result,tenames,labelslist).to_csv('submit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8393e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'resnet18.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
